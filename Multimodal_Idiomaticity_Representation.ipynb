{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "na3IV3XLfKGC"
   },
   "source": [
    "# Multimodal Idiomaticity Representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4VTkMBMn0FHF",
    "outputId": "a0ce45a0-e9aa-4129-896b-50b0f921aa2d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFQYBpWr0QIN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Concatenate,Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score,accuracy_score, precision_score, recall_score\n",
    "from transformers import pipeline, T5Tokenizer, T5ForConditionalGeneration, TFBertModel, BertTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9BrBb3BEjwC"
   },
   "outputs": [],
   "source": [
    "#Laoding csv files\n",
    "train_df = pd.read_csv('/content/drive/MyDrive/train/train.csv')\n",
    "val_df = pd.read_csv('/content/drive/MyDrive/val/val.csv')\n",
    "test_df = pd.read_csv('/content/drive/MyDrive/test/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "is-lu8UlEruN",
    "outputId": "fae08204-ca4c-4816-9271-b1d5a5f97a38"
   },
   "outputs": [],
   "source": [
    "# show the data\n",
    "print (test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "id": "kWDYogOeEve3",
    "outputId": "3da1a05c-5c46-47f2-eb14-a387d4cef3df"
   },
   "outputs": [],
   "source": [
    "# Visualise dataset\n",
    "image_folder = \"/content/drive/MyDrive/test/images\"\n",
    "\n",
    "def visualise_data(df, image_folder):\n",
    "    fig, axes = plt.subplots(len(df)//6+1, 6, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        image_path = os.path.join(image_folder, row['image_name'])\n",
    "        try:\n",
    "            img = Image.open(image_path)\n",
    "            ax = axes[idx]\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(f\"Compound: {row['compound']}\\nLabel: {row['label']}\")\n",
    "            ax.axis('off')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "\n",
    "    # Hide any empty subplots (those beyond the dataset size)\n",
    "    for idx in range(len(df), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualise_data(test_df, image_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvRYoJZxE07c"
   },
   "source": [
    "##  NLP Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypybtVgZ0Sry"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Checking and cleaning dataframe from duplicates,checking that each sentence has exatky one correct label and has fixed number of rows(3)\n",
    "def clean_dataframe(df, expected_rows_per_sentence=3):\n",
    "    df = df.drop_duplicates(subset=['sentence', 'image_name'])\n",
    "    label_counts = df.groupby('sentence')['label'].sum()\n",
    "    valid_sentences = label_counts[label_counts == 1].index\n",
    "    df = df[df['sentence'].isin(valid_sentences)]\n",
    "    row_counts = df.groupby('sentence').size()\n",
    "    valid_sentences = row_counts[row_counts == expected_rows_per_sentence].index\n",
    "    return df[df['sentence'].isin(valid_sentences)]\n",
    "\n",
    "\n",
    "train_df = clean_dataframe(train_df)\n",
    "val_df = clean_dataframe(val_df)\n",
    "test_df = clean_dataframe(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiPse2uZFUED"
   },
   "source": [
    "# LLM based Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYppKBAffwRI"
   },
   "source": [
    "The data aumentation is performed on the training dataset by generating paraphrased versions of the 'sentence' and 'image_caption' columns.\n",
    "\n",
    "\n",
    "*   using an LLM (T5) to generate paraphrases\n",
    "*   unisng a sentence embedding model (Sentence-BERT) to filter them, ensuring they remain semantically close to the originals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671,
     "referenced_widgets": [
      "2cb681f3a4204dd18c8db1e4d161f70c",
      "50f3fd6f289a4200868a7dbd829be5c2",
      "20e18f02596441d3b487a50c5a73e9b3",
      "58f5e9bcbcd049fda3fb34e7455d3800",
      "f00d09b87629467d9e68016d3fcc539c",
      "c3e177696d28442094b9ff97169524a3",
      "97f80eb021bf4331a98ace9a7dfd496e",
      "fe4e6ec72c6e4f93be2d0ab7b089ff76",
      "9d1bad84da0d495e83199bafcaa9724c",
      "9ebf21a8472e4682bd7117eaffdaa82f",
      "7c7f106136d44acf859b142a053189c0",
      "4f02f67bbdfe42bab94c2383b38774eb",
      "563bfb0f724643fb8bfedcdbe22a3ad2",
      "cb327e8a8f5b43528fdeae7d2b9f0430",
      "f4d792bcf118459eb5a8697fe04640eb",
      "7848ff419cd24d76b3c8f08cc39630df",
      "97a9e367ee7a4fe689db467f6469543e",
      "1b309f019152404fad569a902e8862d6",
      "87bc0e6daaed4647808954329a2c8844",
      "ea5354bd6fe14db9ab9596e63cf82054",
      "7df430b012b94d24b04c12ba723312f1",
      "e52a6a4f57fa490a97fa098fdeffe369",
      "4a5c624826ba45f5a02b620da5899029",
      "36c6311ffac44c7da7ffae5e4b57adff",
      "41aeb0a708d34101bc63985fe052a35a",
      "a75fb3ef91ba4cecae5d6eea7e285b7a",
      "7f7e932727be42d49ec577b359807dc3",
      "e302639aaf4a4eeb9f479ffcbd91ec29",
      "f48ab9cce492426595998b737dd00961",
      "0f7eaa2fb4294ecc9d02e2a4a5541ff2",
      "f0982b7151604a7ea9c77265c0c6e091",
      "4e10aebe09d2471b94ea64f8c56cc3c1",
      "510f3a1b887c427bb51d670d1ce5ae85",
      "b7735cd8f0c0461a8bcaddb7fa993878",
      "bc80c6a6e2e946eca561727817c1b51c",
      "c24c56ab15584e18a7568410af4f8898",
      "532f2b5024264bc2bc8d6ed1a0b4ccdf",
      "3dee1910c5fe4fde846728c7bb974217",
      "e0d336791fb841309bc141d963f89ecb",
      "62f8b29581384a0ba9ab0e0f450ca508",
      "8b62ab30d86d40d2a9f3c450d17fb919",
      "b9afdad44af2432db82e1db996750cde",
      "65415b50286f4434981eed9f857c069a",
      "c21fc94b164f4ead9f939f494a0eb178",
      "2cfbf75e4c324ca7832411a54935d4ba",
      "f8e1874c32ab42449db28b5fa4d108b8",
      "48be7aad06d443a5a77fd81c4be38132",
      "248561b51ea147068a433c3602daa214",
      "dd391093100a46c8a5c93e627fc25f7c",
      "616d1f23a39945ea84b9c39659177e4b",
      "0fe8feb9e12743e9b465786ddfd68106",
      "eee2f2089ed4419b93b20def6d5f812e",
      "b59d455ac42b484ba01999eea87358af",
      "2ec61d2fda984bbaabd4350791905c5f",
      "5293125a642e4e7fbd4be78ccf7bab6f",
      "db097afb9fcd42508875e343c487419d",
      "c0db444a5f8748a098b687d3982351c2",
      "49264073923b40998870fe5474fbcb70",
      "d5098481047544ac91852c36329ffde5",
      "9b229bd1b8b64339b6cd9f62c3794700",
      "fccf020bdc684eeab7b522bf76464031",
      "ff4b00dc44bd4252b16895d70a0bad7c",
      "34dec093cf464dc29f2e379cd71e27da",
      "a6595463db994cb595c8c39d3657a85d",
      "995efdbc657f49bdb2774cdbdfba915e",
      "86fa0d6ff5cd4e79823c4bb9dd9ce523",
      "9866a8b8ddf84647ad78dcbcd3e29c8e",
      "828316fcd7f44fbe83f1f1997e369fdf",
      "5f81f4c6e80046a2b51f41a47641f20f",
      "b78dc0b61ee64ede83ed6732830ef2c5",
      "95b161da8d01427f8abbbe52a0f0fb78",
      "cba09f0b1d4447289f5b72132f505485",
      "251c8e76fb4b41e3a5da8a185c68be3f",
      "c50e9ccc354046eaac106c7f08ec231e",
      "52e2ebe70bd841f294566bb8bf808376",
      "92c91e518a0047c79d711db81e24140c",
      "36f2dd3823ab4afd81412d624f05be23",
      "e3a94d97ab3a456e811703a51dbc40a3",
      "50f5317cd72d48aa809dcb13d610322d",
      "576e486a8697457083f251bcfd070eab",
      "8ea87f48043c40f689b91a010e4167d4",
      "f67a1086b58642c49bcf130d2f864aff",
      "2bb682d64de0449eaf54a79721809a11",
      "d3141fd9f4f54a1f9e2ada977e4357a6",
      "2870040f9aec4944893d4b5bf51de2ab",
      "8a45446e0ed04ceda7ecbe3e1a522bf4",
      "4e1d809354b546578276a3577dfc5643",
      "9092b0a479354760b6a000c1d370cefc",
      "58298f293aeb45beae397e8a2c446451",
      "4573fbac3b204be296febeb778cd7a25",
      "0870a4237c0647f6a4a67e05b7625504",
      "48adc22322904b48985e95a9be042ce8",
      "5c4e704c72a04c0bb7e6921074fa1cac",
      "e29ba2690956466d81cab1e832bb82bf",
      "4717ab17098644339e88e24b59c747f6",
      "3eec2ba5fd8a4f498cfb996275f73239",
      "0fab0138c9254d62afa3dd7e260e84c1",
      "48307e9c2541411f8d2fb5ba05497972",
      "2334473f3473452db4674ee9220de84c",
      "f3dc9413e67f43b0b45ef46801645e45",
      "47aff2cd285f43f3976a71ba183667e8",
      "aabc77747fb14d37b4eff24cc146885d",
      "624ed27b200f4f9f8ffe1bc60e0360a1",
      "4ad9474135664697b18fb5cabb190258",
      "ae5add5e60454b7897168bd5002c19b0",
      "28dcf9a1b9944d11913f4ebf23f277e5",
      "1774a6a341ff47888c70189737c9d659",
      "74c0699096484ff48c435f31ad7bcf68",
      "17a3717a490545319dfffec8935203f8",
      "53cb3b62b4d9443f9997d83a751b9a56",
      "6ebc2204e55841a9a30870dcd8007836",
      "07233572305c41458f94467bef5b9ab7",
      "a65e3cd772024aa2a276017663bfe3b1",
      "c34ce9e978ec4eac850ed436fd37144a",
      "cb6c981642c84f1e920772d3df8295af",
      "5df2a7faae75426faace8bf604761cf5",
      "7b4233bb5a234cd698db841aac8ab984",
      "01b2f9a5a7974620b0ac228e8d0724c2",
      "ad3627c6c7c94bcfb2b5d3992c14b446",
      "278b03e0703343159077748169c6f787",
      "2b4de3ecb19541ff92de5369a96954fd",
      "a20c5de26e7f4d4bbf57b75d577b6fd0",
      "faddb7e3f65c4062beae63e748ba0548",
      "036ac4bebfbb4259a99bf13ad2298612",
      "7fa69c6b9dea467eb030ca664544fcef",
      "c4f5521e7b3442e682dbe4e25a7ea078",
      "3d048ed5ea884127ae760386953cae82",
      "7552a4de73ad4222974b094e7b27dc30",
      "7c0b6584b5484e58af24022549053324",
      "035783ca07a445ee957c615b06880aaa",
      "d2bab8f06b8e47cfb65ee58d54bdd1e7",
      "b976aa190ed64ca291de3c9d6f1ed59a",
      "516d158112014168b9743239a7d7b586",
      "d912e88b81c54ddab8faf82856f5b8c3",
      "0793acdbc32e4f27aff679167b643d81",
      "88cc42523d804bb89f2829f380dae163",
      "4ac26cbcc9214d25ae6283224943c98c",
      "8e3136d716fd4ec490e349d26adac9ac",
      "dd6576e31b1747a0b0eb340f56b6b803",
      "edc2299c140a4755aa191b05e766111f",
      "92449d5f6c774e19975d95b332c46e7d",
      "028e0681822e49d2bd0a42856e1ed4b7",
      "7c4ab543f9444a1ea318b9dac70b8429",
      "7648d2348ff14299a0a3fa28b528fb25",
      "3d5c12eda8bc446589baf65de4dd0581",
      "e1068a3ca2bd4e5dbd547432713906a1",
      "5e082d41e9b8491ba5afbd37f0869c64",
      "44197fc4b38a480bbbbd5dd39ef85f91",
      "055410cd527d4bfb92b399c591ede044",
      "ce85a3b25f1e48a882961c943d4ade13",
      "6d6de268fc6948faad7077cf1b91c52a",
      "18009ddac78445108b7ba0f7044767b1",
      "4f63e8bb6f2a4e5fab63f2291ecc6fac",
      "14b601ff981745379b34af9d6fd80a65",
      "5fe77195c5574e8d9e94d1901a2dd914",
      "8ddeb7c09eda49919979e52eccdb098d",
      "d635433569fc4329a4531f375ee8309f",
      "d4645635f4594798bff24457666edcc6",
      "e23023a52f4a4a3183246d9dfdb7245c",
      "fad701f5efe14ea39b52d97e4581a64b",
      "e04b99e763ff4e03abcbad2c427c95ba",
      "aa14366cc5724c078727d5d98f849b61",
      "e815d94464a14c3682d0db296a34d5aa",
      "e6d6fa550a2c4cfd8e06eb15d675cf3c",
      "744d0fead9774fb7b41a1a75c9a6ff22",
      "0e3edbf8139641f0941b5f7b34d4e36e",
      "c61cfd72b67d497cb72d2845daa30ee5",
      "153dbc3b43024b5498ef60f8e5503f40",
      "53e06691cac547aa87e95e322b143a4d",
      "1d1df4da04a94a96b6ede212ba1e2e90",
      "87cf1fd5817b4aa3856a7f6b4ae42e88",
      "f941245010ff4f1fb7aee48564d01d49",
      "eef66764aed949ce8153dddf5454272d",
      "291c7c38d60b48ddbb846b8d08b71beb",
      "6083fd453fd0472fba053b006208f611",
      "50f30f5dc942495b9960d5a42946af09"
     ]
    },
    "id": "vW8Iho_00aR9",
    "outputId": "6d7991a0-eb4e-48fa-c588-7b9c4c986491"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def data_augmentation_llm(df, num_augmentations=2, similarity_threshold=0.8):\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "    similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    augmented_data = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        sentence = row['sentence']\n",
    "        caption = row['image_caption']\n",
    "        input_text = f\"paraphrase: {sentence}\"\n",
    "\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n",
    "        outputs = model.generate(input_ids, max_length=50, num_return_sequences=num_augmentations, num_beams=5)\n",
    "        aug_sentences = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        sentence_embedding = similarity_model.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "        for aug_sentence in aug_sentences:\n",
    "            aug_embedding = similarity_model.encode(aug_sentence, convert_to_tensor=True)\n",
    "            similarity = util.pytorch_cos_sim(sentence_embedding, aug_embedding).item()\n",
    "            if similarity >= similarity_threshold:\n",
    "                new_row = row.copy()\n",
    "                new_row['sentence'] = aug_sentence\n",
    "                augmented_data.append(new_row)\n",
    "\n",
    "        #Caption paraphrasing\n",
    "        input_text = f\"paraphrase: {caption}\"\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n",
    "        outputs = model.generate(input_ids, max_length=50, num_return_sequences=num_augmentations, num_beams=5)\n",
    "        aug_captions = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        caption_embedding = similarity_model.encode(caption, convert_to_tensor=True)\n",
    "\n",
    "        for aug_caption in aug_captions:\n",
    "            aug_embedding = similarity_model.encode(aug_caption, convert_to_tensor=True)\n",
    "            similarity = util.pytorch_cos_sim(caption_embedding, aug_embedding).item()\n",
    "            if similarity >= similarity_threshold:\n",
    "                new_row = row.copy()\n",
    "                new_row['image_caption'] = aug_caption\n",
    "                augmented_data.append(new_row)\n",
    "\n",
    "    return pd.concat([df, pd.DataFrame(augmented_data)], ignore_index=True)\n",
    "\n",
    "\n",
    "train_df_augmented = data_augmentation_llm(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owYRVQqNFZiL"
   },
   "source": [
    "# Data Preprocessing and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URdju2UOgU_9"
   },
   "source": [
    "\n",
    "Tokenizing Text: Converting the 'sentence' and 'image_caption' columns from\n",
    "\n",
    "*   Tokenizing Text: Converting the 'sentence' and 'image_caption' columns from train_df_augmented into numerical sequences (token IDs and attention masks) that BERT can process.\n",
    "*   Preprocessing Images: Loading and preprocessing images  for input to EfficientNetB0.\n",
    "*   Extracting Labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpCitzYb0gjI"
   },
   "outputs": [],
   "source": [
    "# preparing Bert tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "def bert_text_to_sequence(texts, max_len=50):\n",
    "    encodings = bert_tokenizer(texts, truncation=True, padding='max_length', max_length=max_len, return_tensors='tf')\n",
    "    return {'input_ids': encodings['input_ids'], 'attention_mask': encodings['attention_mask']}\n",
    "\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(image_name, base_folder):\n",
    "    image_path = f'{base_folder}/images/{image_name}'\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    img = img_to_array(img)\n",
    "    img = tf.keras.applications.efficientnet.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "#training data#\n",
    "#tokenizing sentences\n",
    "train_encodings = bert_text_to_sequence(train_df_augmented['sentence'].tolist())\n",
    "#tokenizing captions\n",
    "train_caption_encodings = bert_text_to_sequence(train_df_augmented['image_caption'].tolist())\n",
    "#preparing images\n",
    "images_train = np.array([load_and_preprocess_image(id,  '/content/drive/MyDrive/train') for id in train_df_augmented['image_name']])\n",
    "#extracing labels\n",
    "labels_train = train_df_augmented['label'].values\n",
    "\n",
    "#validation data#\n",
    "val_encodings = bert_text_to_sequence(val_df['sentence'].tolist())\n",
    "val_caption_encodings = bert_text_to_sequence(val_df['image_caption'].tolist())\n",
    "images_val = np.array([load_and_preprocess_image(id,  '/content/drive/MyDrive/val') for id in val_df['image_name']])\n",
    "labels_val = val_df['label'].values\n",
    "\n",
    "#test data#\n",
    "test_encodings = bert_text_to_sequence(test_df['sentence'].tolist())\n",
    "test_caption_encodings = bert_text_to_sequence(test_df['image_caption'].tolist())\n",
    "images_test = np.array([load_and_preprocess_image(id,  '/content/drive/MyDrive/test') for id in test_df['image_name']])\n",
    "labels_test = test_df['label'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3UzwCfdFouE"
   },
   "source": [
    "# Zero-shot prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-he3Kz3Mg2oE"
   },
   "source": [
    "\n",
    "*   Perform Zero-shot Classification: Using a pre-trained natural language inference (NLI) model to evaluate how well an image caption matches a sentence, without training on the dataset.\n",
    "*   (zero_shot_probs) will be combined with model predictions\n",
    "\n",
    "*   Evaluate Relevance: For each sentence-caption pair in test_df, it determines the likelihood that the caption describes the sentence’s intended image.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244,
     "referenced_widgets": [
      "c677d08f19734fe8a720a7f045a7bb21",
      "ba3ec5ecc5064fdfac39efd385beea79",
      "16eadc75187046b8b40ebd5b44203478",
      "83b652a4ef81404d9e369f46d8b61ddc",
      "a35220829a204ee58150840f914a1e95",
      "f1a8253afaaf4c87b8312081f0887a3f",
      "c053fa00c2324fec88abcf5afe1a0c83",
      "b5840ebe7b83420bb61e8b559a0a4143",
      "6d5ce3739eb24e23ba8fc35959a1c2c6",
      "1745c8f4a6ff4062bdd668aad67bedaf",
      "343722427632451bbf06f281485a1486",
      "3c78c755c164481dae1bd9b73d8ebddd",
      "7877eeb89e394571831862aee2099b92",
      "bbce4acbc45148eea856a1d85bda976d",
      "d963cce4448f4e0e8f77d8e823e47722",
      "c03e4c1a6ce8456e8082e2b924a9f7f7",
      "8dc27022f999447681c4b75aacb432cf",
      "120986f38d21408bb4e5bb5f0c6699d5",
      "d67a2286bf564465bcc7e543f63bd128",
      "cae20756c9f74df3ad1b8cd3e447fbc8",
      "4929c9b18ab34b6ebcda15f04edd5dbf",
      "9e8665c20c44420fbb8fdfbe523e790c",
      "a81f33c486904386ac21b4639ecadcd0",
      "69aaea64756f429c9d954b5cb7d2ba99",
      "88db179a66e74a0d8709804e0eb1c1d9",
      "160b8e8690934df29fdb83b6d4aeb3d1",
      "7cb1ac27056c463099cd22c523597fab",
      "74b4a89378f04767a2f09f1023d962b8",
      "5b74495f73d646cdb6855e0ccedaf1d1",
      "0a22710ec2fd420aacab7580b2604c2d",
      "1f0b5f806d9a41328a828a980153e6c4",
      "03a87e01bc1447d9b76ead9447d544ef",
      "73890092895640c080f8310eeb42fcbe",
      "f5a9a91a438f48d6808a217a10e65408",
      "cb8ffc52bf8c4463b94311d4bc0238d7",
      "a1c295c145c04d45be804f14b282273a",
      "2cdb9c1dc1c4457ab131e3552e103db9",
      "0a9058eef252492b8f3b76c08604da2a",
      "e62558a0c91d4110a9f8684004f2c67f",
      "27507af6bb014991b82f2bb04170c0da",
      "070232adbaa84db387157bb8954d1914",
      "6d053da3482d4aee83a852b1335864bf",
      "33482407a1e04a5fbf408284e28f4348",
      "aca15e94096748f5be484d924877ac22",
      "3366c18582784638a573bf0b013623ea",
      "341f0a81b6f347de876e5287e10ef123",
      "73dd7b7e21414a3b8594177e5aaea26a",
      "c4831d3562d4408cae3ed542c0cbf055",
      "d4c4ee7b5278452f9ce7fab52fe183a9",
      "5bc7f6c9fef44d3f99d9656e080c8951",
      "700e76a255484bf9a9aa4a241e959dfe",
      "0de72923cf3242bd9b5c0d507dcc6b3d",
      "801b8bc95ba64c8eab124e9d360f48d8",
      "cddeb0d6c02542fdbc62b3a2f80ddbc3",
      "b1d5ff00fada40e4bf0e54727212a3fc",
      "d1dba180a7224e8bac6c40258d191233",
      "b27ca65e763c4861b28960aaa8ae97fb",
      "308a3a3e501b484b967ec73def6ffa48",
      "b4497e2e43fd49478c1426bf0b7dab49",
      "9649a201281e43dca16fa048937d10a9",
      "21935c9c42494b89a863770bcb67cf18",
      "746948f6acd545598e95c437daaf0f68",
      "e4eb6e86a6fd4b7dafcb04b1dd622a6c",
      "6afcc0232b96464cb8954a04dfd20d18",
      "4211d5fa033348578d685170458c980e",
      "feae00c390d6454ba2d81d72ca949a35"
     ]
    },
    "id": "3uFtPYDn0jEY",
    "outputId": "925c2617-46ea-4521-8f0b-bade196f96e8"
   },
   "outputs": [],
   "source": [
    "def zero_shot_prediction(df):\n",
    "\n",
    "    classifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\n",
    "    predictions = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        sentence = row['sentence']\n",
    "        caption = row['image_caption']\n",
    "        result = classifier(sentence, candidate_labels=[caption, \"irrelevant\"], hypothesis_template=\"This Sentence means {}.\")\n",
    "        predictions.append(result['scores'][0])\n",
    "    return np.array(predictions)\n",
    "\n",
    "#using the probability obtained by zero_shot to be combined with the model's probablity to shape the overall probability\n",
    "zero_shot_probs = zero_shot_prediction(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOh0QtyjFzNN"
   },
   "source": [
    "# Fine-tuning based and other prediction approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6i-kLmGjUTU"
   },
   "source": [
    "Keras uses symbolic tensors (KerasTensor) during model building, but TFBertModel expects concrete tensors (tf.Tensor). The custom layer handles this gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAKfvq1S0lG7"
   },
   "outputs": [],
   "source": [
    "# Custom Keras layer to wrap TFBertModel\n",
    "#this is a custom Keras layer that wraps TFBertModel (TensorFlow version of BERT ) to make it compatible with Keras\n",
    "# Without this wrapper,directly calling TFBertModel within a Keras model can lead to errors because of tensor type mismatches ( KerasTensor vs. tf.Tensor)\n",
    "class BertLayer(Layer):\n",
    "    def __init__(self, bert_model, **kwargs):\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "        self.bert_model = bert_model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs is a dictionary with 'input_ids' and 'attention_mask'\n",
    "        bert_output = self.bert_model(inputs)[0]\n",
    "        return bert_output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(BertLayer, self).get_config()\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156,
     "referenced_widgets": [
      "e488780dc5a249a1bb553feccd819002",
      "666062f5982b4d2f85a8cbe7c7ca1644",
      "b2f945dd98b44135b2e66e8aa6f36254",
      "188253fd541a4f70b710086f5a29aee7",
      "6bf0a546752f4d99a9399db6c45ee0a4",
      "10549981cf2846978bd4646902131054",
      "997d71eaa23f427aaf5de4c5305a1be2",
      "d9303a2d64124d7686d09c8bb05c4fa9",
      "38a12ec71fce4366bd1bdf6b7a66ce6d",
      "a1bce5e2cdc9453388b00b41e8cdb550",
      "4a59170f706740359c270e59049559db"
     ]
    },
    "id": "uznzE2NB0nB_",
    "outputId": "78ddc063-a626-4b69-c053-8231a364e744"
   },
   "outputs": [],
   "source": [
    "projection_dim = 128\n",
    "\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def text_encoder(inputs, name):\n",
    "    # inputs is a dictionary with 'input_ids' and 'attention_mask'\n",
    "    bert_layer = BertLayer(bert_model, name=name)\n",
    "    bert_output = bert_layer(inputs)\n",
    "    text_embedding = tf.keras.layers.GlobalAveragePooling1D()(bert_output)\n",
    "    return text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IwpfgAHG0pp_",
    "outputId": "5fce5539-35ca-4dfa-922d-a324bdd4f257"
   },
   "outputs": [],
   "source": [
    "# model inputs\n",
    "context_input_ids = Input(shape=(50,), name='context_input_ids', dtype=tf.int32)\n",
    "context_attention_mask = Input(shape=(50,), name='context_attention_mask', dtype=tf.int32)\n",
    "caption_input_ids = Input(shape=(50,), name='caption_input_ids', dtype=tf.int32)\n",
    "caption_attention_mask = Input(shape=(50,), name='caption_attention_mask', dtype=tf.int32)\n",
    "image_input = Input(shape=(224, 224, 3), name='image_input')\n",
    "\n",
    "# encoding texts\n",
    "context_inputs = {'input_ids': context_input_ids, 'attention_mask': context_attention_mask}\n",
    "caption_inputs = {'input_ids': caption_input_ids, 'attention_mask': caption_attention_mask}\n",
    "context_emb = text_encoder(context_inputs, name='context_encoder')\n",
    "caption_emb = text_encoder(caption_inputs, name='caption_encoder')\n",
    "\n",
    "#encoding image with EfficientNetB0\n",
    "efficientnet = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=image_input)\n",
    "image_features = GlobalAveragePooling2D()(efficientnet.output)\n",
    "image_proj = Dense(projection_dim, activation='relu')(image_features)\n",
    "\n",
    "# combining image and caption embeddings\n",
    "# merging the image and caption embeddings to represent the image-caption pair as a single vector.\n",
    "# combining visual and textual information about the image, capturing their joint representation for comparison with the sentence.\n",
    "image_caption_emb = Concatenate()([image_proj, caption_emb])\n",
    "\n",
    "# Project embeddings to a common dimension\n",
    "context_proj = Dense(projection_dim, activation='relu')(context_emb)\n",
    "image_caption_proj = Dense(projection_dim, activation='relu')(image_caption_emb)\n",
    "\n",
    "#combining all modalities\n",
    "combined = Concatenate()([context_proj, image_caption_proj])\n",
    "#combineing all information and maps it to the binary classification task\n",
    "output = Dense(1, activation='sigmoid')(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eq1GhAXO0r7G"
   },
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    inputs=[context_input_ids, context_attention_mask, caption_input_ids, caption_attention_mask, image_input],\n",
    "    outputs=output\n",
    ")\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xOB5aAD00uFK",
    "outputId": "303c809f-1b26-432e-a7a4-2c7c531033a2"
   },
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit(\n",
    "    [train_encodings['input_ids'], train_encodings['attention_mask'],\n",
    "     train_caption_encodings['input_ids'], train_caption_encodings['attention_mask'], images_train],\n",
    "    labels_train,\n",
    "    validation_data=([val_encodings['input_ids'], val_encodings['attention_mask'],\n",
    "                      val_caption_encodings['input_ids'], val_caption_encodings['attention_mask'], images_val], labels_val),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[lr_callback, early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DiIJSrq0zjS",
    "outputId": "2319a995-1e0a-40b1-e184-a520ad46bec7"
   },
   "outputs": [],
   "source": [
    "# prediction on test set\n",
    "test_probs = model.predict(\n",
    "    [test_encodings['input_ids'], test_encodings['attention_mask'],\n",
    "     test_caption_encodings['input_ids'], test_caption_encodings['attention_mask'], images_test]\n",
    ")\n",
    "\n",
    "#ensuring test_probs and zero_shot_probs is a 1D array\n",
    "test_probs = test_probs.squeeze()\n",
    "zero_shot_probs = zero_shot_probs.squeeze()\n",
    "\n",
    "# ensembling with zero-shot predictions\n",
    "ensemble_probs = 0.7 * test_probs + 0.3 * zero_shot_probs\n",
    "\n",
    "test_df['prob'] = ensemble_probs\n",
    "predicted = test_df.groupby('sentence').apply(lambda x: x['image_name'].iloc[x['prob'].argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rOYc5glkcrd"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pS-sjbIA05mv",
    "outputId": "1c654190-fb1e-47fa-f23b-070332aec754"
   },
   "outputs": [],
   "source": [
    "# Get true image names where label == 1\n",
    "true = test_df[test_df['label'] == 1].set_index('sentence')['image_name']\n",
    "\n",
    "predicted_labels = (ensemble_probs > 0.5).astype(int).flatten()\n",
    "f1 = f1_score(labels_test, predicted_labels, average='macro')\n",
    "precision= precision_score(labels_test, predicted_labels, average='macro')\n",
    "recall= recall_score(labels_test, predicted_labels, average='macro')\n",
    "accuracy=accuracy_score(labels_test, predicted_labels)\n",
    "\n",
    "print(f'Test F1 Score       : {f1:.4f}')\n",
    "print(f'Test Precision Score: {precision:.4f}')\n",
    "print(f'Test Recall Score   : {recall:.4f}')\n",
    "print(f'Test Accuracy Score : {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZa-i5_akr2_"
   },
   "source": [
    "# Displaying True and Predicted Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "s3_TrJCI08bb",
    "outputId": "71c6e72c-aacb-4306-b607-0903cf3df2a1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_image_for_display(image_name, base_folder):\n",
    "    image_path = f'{base_folder}/images/{image_name}'\n",
    "    return load_img(image_path)\n",
    "\n",
    "\n",
    "\n",
    "def display_predicted_vs_true(test_df, predicted, true, base_folder='/content/drive/MyDrive/test', num_samples=5):\n",
    "    pred_df = pd.DataFrame({'sentence': predicted.index, 'pred_image_name': predicted.values})\n",
    "    true_df = pd.DataFrame({'sentence': true.index, 'true_image_name': true.values})\n",
    "    merged_df = pred_df.merge(true_df, on='sentence', how='inner')\n",
    "    merged_df = merged_df.merge(test_df[['sentence', 'image_name', 'image_caption']],\n",
    "                                left_on=['sentence', 'pred_image_name'],\n",
    "                                right_on=['sentence', 'image_name'],\n",
    "                                how='left',\n",
    "                                suffixes=('', '_pred'))\n",
    "    merged_df = merged_df.merge(test_df[['sentence', 'image_name', 'image_caption']],\n",
    "                                left_on=['sentence', 'true_image_name'],\n",
    "                                right_on=['sentence', 'image_name'],\n",
    "                                how='left',\n",
    "                                suffixes=('_pred', '_true'))\n",
    "\n",
    "    sample_df = merged_df.head(num_samples)\n",
    "\n",
    "    for idx, row in sample_df.iterrows():\n",
    "        pred_image = load_image_for_display(row['pred_image_name'], base_folder)\n",
    "        true_image = load_image_for_display(row['true_image_name'], base_folder)\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 12))\n",
    "\n",
    "        # Predicted images\n",
    "        ax1.imshow(pred_image)\n",
    "        ax1.set_title('Predicted')\n",
    "        ax1.axis('off')\n",
    "        pred_text = f\"Sentence: {row['sentence']}\\nCaption: {row['image_caption_pred']}\"\n",
    "        ax1.text(0.5, -0.1, pred_text, ha='center', va='top', transform=ax1.transAxes, wrap=True)\n",
    "\n",
    "        # True images\n",
    "        ax2.imshow(true_image)\n",
    "        ax2.set_title('True')\n",
    "        ax2.axis('off')\n",
    "        true_text = f\"Sentence: {row['sentence']}\\nCaption: {row['image_caption_true']}\"\n",
    "        ax2.text(0.5, -0.1, true_text, ha='center', va='top', transform=ax2.transAxes, wrap=True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "display_predicted_vs_true(test_df, predicted, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1tc-xMVxDw3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
